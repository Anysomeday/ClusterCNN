{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import spectral\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, adjusted_rand_score\n",
    "from tensorflow import keras\n",
    "\n",
    "def applyPCA(X, numComponents=75):\n",
    "    xshape = X.shape\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    PCs = pca.fit_transform(X.reshape(-1, xshape[2])).reshape(xshape[0], xshape[1], numComponents)\n",
    "    return PCs\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2*margin, X.shape[1] + 2*margin, X.shape[2]))\n",
    "    newX[margin:X.shape[0] + margin, margin:X.shape[1] + margin, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "def count_FLOPs(model):\n",
    "    '''FLOPs的计算：    \n",
    "    参考https://www.zhihu.com/question/65305385\n",
    "    以及https://blog.csdn.net/weixin_43915709/article/details/94566125\n",
    "    '''\n",
    "    FLOPs = 0\n",
    "    for i in range(len(model.layers)):\n",
    "        layer = model.layers[i]\n",
    "        if layer.__class__.__name__ == 'Conv3D':\n",
    "            FLOPs += 2*np.prod(layer.kernel_size[:3])*np.prod(layer.output_shape[1:5])\n",
    "        if layer.__class__.__name__ == 'Conv2D':\n",
    "            FLOPs += 2*np.prod(layer.kernel_size[:2])*layer.input_shape[3]*np.prod(layer.output_shape[1:4])\n",
    "        if layer.__class__.__name__ in ('MaxPooling3D', 'AveragePooling3D'):\n",
    "            FLOPs += np.prod(layer.pool_size[:3])*np.prod(layer.output_shape[1:5])\n",
    "        if layer.__class__.__name__ in ('MaxPooling2D', 'AveragePooling2D'):\n",
    "            FLOPs += np.prod(layer.pool_size[:2])*np.prod(layer.output_shape[1:4])\n",
    "        if layer.__class__.__name__ == 'GlobalAveragePooling3D':\n",
    "            FLOPs += np.prod(layer.input_shape[1:5])\n",
    "        if layer.__class__.__name__ == 'GlobalAveragePooling2D':\n",
    "            FLOPs += np.prod(layer.input_shape[1:4])             \n",
    "        if layer.__class__.__name__ in ('Activation', 'Add', 'Multiply'):\n",
    "            FLOPs += np.prod(layer.output_shape[1:])            \n",
    "        if layer.__class__.__name__ == 'Dense':\n",
    "            FLOPs += 2*layer.input_shape[1]*layer.output_shape[1]\n",
    "        #if layer.__class__.__name__ == 'BatchNormalization':               \n",
    "    return FLOPs\n",
    "\n",
    "HSI = sio.loadmat(os.path.join(os.getcwd(),'data/PaviaU.mat'))['paviaU']\n",
    "GT = sio.loadmat(os.path.join(os.getcwd(),'data/PaviaU_gt.mat'))['paviaU_gt']\n",
    "test_ratio = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original images\n",
    "#view=spectral.imshow(HSI, (53, 31, 8), figsize=(5, 5), classes=GT, title='Pavia University')\n",
    "\n",
    "# lulc maps\n",
    "predictions = np.argmax(model.predict(X), axis=1)+1\n",
    "outputs = np.zeros(GT.shape)\n",
    "k = 0\n",
    "for i in range(GT.shape[0]):\n",
    "    for j in range(GT.shape[1]):\n",
    "        if int(GT[i,j]) == 0 :\n",
    "            continue\n",
    "        else:\n",
    "            outputs[i, j] = predictions[k]\n",
    "            k += 1\n",
    "            \n",
    "#ground_truth = spectral.imshow(classes = GT,figsize =(5,5), title='Ground Truth')\n",
    "#spectral.save_rgb(str(dataset)+\"_ground_truth.jpg\", GT, colors=spectral.spy_colors)\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5), title='SSRN')\n",
    "#spectral.save_rgb(str(dataset)+\"_ClusterCNN_\"+expnum+\".jpg\", outputs.astype(int), colors=spectral.spy_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# DCCNN\n",
    "https://github.com/stop68/Remote-Sensing-Image-Classification/blob/master/demo_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n",
      "DBMA: 0.9857398893477752 0.9811037280460545  Time Comsumption: 365\n"
     ]
    }
   ],
   "source": [
    "from Utils import networks as nw\n",
    "\n",
    "patch = 5  # if patch==5, WCRN DCCNN. if patch==7, BDMA.\n",
    "im = applyPCA(HSI,numComponents=15)\n",
    "im1x,im1y,im1z = im.shape\n",
    "cls1 = np.max(GT)\n",
    "\n",
    "X, y = createImageCubes(im, GT, windowSize=patch)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345, stratify=y)\n",
    "# x_train,y_train = rscls.make_sample(x_train,y_train)  # augmentation\n",
    "y_train = keras.utils.to_categorical(np.asarray(y_train))\n",
    "\n",
    "# DBMA\n",
    "model = nw.DCCNN(im1z, patch, cls1) \n",
    "adadelta1 = keras.optimizers.Adadelta(lr=1.0)\n",
    "adadelta2 = keras.optimizers.Adadelta(lr=0.2)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=2)\n",
    "\n",
    "for i in range(1):\n",
    "    time1 = int(time.time())\n",
    "    # first train the model with lr=1.0    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adadelta1,metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train,batch_size=20,epochs=80,verbose=0,shuffle=True)\n",
    "\n",
    "    # then train the model with lr=0.1; if loss not decrease for 5 epoches, stop training\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adadelta2,metrics=['accuracy'])   \n",
    "    model.fit(x_train,y_train,batch_size=20,epochs=20,verbose=0,shuffle=0,callbacks=[early_stopping])\n",
    "    time2 = int(time.time())\n",
    "\n",
    "    # predict\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    print('DBMA:', oa, kappa, ' Time Comsumption:', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# DBMA\n",
    "https://github.com/stop68/Remote-Sensing-Image-Classification/blob/master/demo_keras.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBMA: 0.9986233408659965 0.9981756948205219  Time Comsumption: 562\n"
     ]
    }
   ],
   "source": [
    "from Utils import networks as nw\n",
    "\n",
    "patch = 7  # if patch==5, WCRN DCCNN. if patch==7, BDMA. if patch==9, ResNet-avg.\n",
    "im = applyPCA(HSI,numComponents=15)\n",
    "im1x,im1y,im1z = im.shape\n",
    "cls1 = np.max(GT)\n",
    "\n",
    "X, y = createImageCubes(im, GT, windowSize=patch)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], -1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345, stratify=y)\n",
    "# x_train,y_train = rscls.make_sample(x_train,y_train)  # augmentation\n",
    "y_train = keras.utils.to_categorical(np.asarray(y_train))\n",
    "#x_train = x_train.reshape(x_train.shape[0],patch,patch,im1z,-1) # 3D CNN, samples are 5-dimensional\n",
    "#x_test = x_test.reshape(x_test.shape[0],patch,patch,im1z,-1)    # 3D CNN, samples are 5-dimensional\n",
    "\n",
    "# DBMA\n",
    "model = nw.DBMA(im1z, patch, cls1)  # 3D CNN, samples are 5-dimensional\n",
    "adadelta1 = keras.optimizers.Adadelta(lr=1.0)\n",
    "adadelta2 = keras.optimizers.Adadelta(lr=0.2)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=2)\n",
    "\n",
    "for i in range(1):\n",
    "    time1 = int(time.time())\n",
    "    # first train the model with lr=1.0    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adadelta1,metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train,batch_size=20,epochs=80,verbose=0,shuffle=True)\n",
    "\n",
    "    # then train the model with lr=0.1; if loss not decrease for 5 epoches, stop training\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adadelta2,metrics=['accuracy'])   \n",
    "    model.fit(x_train,y_train,batch_size=20,epochs=20,verbose=0,shuffle=0,callbacks=[early_stopping])\n",
    "    time2 = int(time.time())\n",
    "\n",
    "    # predict\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    print('DBMA:', oa, kappa, ' Time Comsumption:', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# SSRN\n",
    "https://github.com/zilongzhong/SSRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import ssrn_SS_IN\n",
    "\n",
    "patch = 7  #25\n",
    "nb_classes = np.max(GT)\n",
    "\n",
    "data_norm = HSI.reshape(np.prod(HSI.shape[:2]),np.prod(HSI.shape[2:]))\n",
    "data_norm = scale(data_norm)\n",
    "data_norm = data_norm.reshape(HSI.shape[0], HSI.shape[1],HSI.shape[2])\n",
    "#im = applyPCA(data_norm,numComponents=15)\n",
    "im = data_norm\n",
    "\n",
    "X, y = createImageCubes(im, GT, windowSize=patch)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345, stratify=y)\n",
    "y_train = keras.utils.to_categorical(np.asarray(y_train))\n",
    "VAL_SIZE = int(0.1*X.shape[0])\n",
    "x_val = x_test[-VAL_SIZE:]\n",
    "y_val = keras.utils.to_categorical(np.asarray(y_test))[-VAL_SIZE:]\n",
    "x_test = x_test[:-VAL_SIZE]\n",
    "y_test = y_test[:-VAL_SIZE]\n",
    "\n",
    "# SSRN 4 with BN\n",
    "model = ssrn_SS_IN.ResnetBuilder.build_resnet_8((1, patch, patch, im.shape[2]), nb_classes)\n",
    "RMS = keras.optimizers.RMSprop(lr=0.0003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMS, metrics=['accuracy'])\n",
    "\n",
    "#x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3], 1)\n",
    "#x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], x_val.shape[3], 1)\n",
    "#x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], x_test.shape[3], 1)\n",
    "\n",
    "for i in range(1):\n",
    "    time1 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x_train, y_train, batch_size=16, epochs=100, shuffle=False, verbose=0)\n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    # predict\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    oa = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    print('SSRN:', oa, kappa, ' Time Comsumption:', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# HybridSN\n",
    "https://github.com/gokriznastic/HybridSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid: 0.984467129016338 0.9794220770793202  Time Comsumption: 375\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "X, y = createImageCubes(HPCs, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "Xtrain = Xtrain.reshape(-1, windowSize, windowSize, PCsNum, 1)\n",
    "Xtest = Xtest.reshape(-1, windowSize, windowSize, PCsNum, 1)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "input_layer = keras.Input((windowSize, windowSize, PCsNum, 1))\n",
    "\n",
    "## convolutional layers\n",
    "conv_layer1 = keras.layers.Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(input_layer)\n",
    "conv_layer2 = keras.layers.Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(conv_layer1)\n",
    "conv_layer3 = keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer2)\n",
    "conv3d_shape = conv_layer3.shape\n",
    "conv_layer3 = keras.layers.Reshape((conv3d_shape[1], conv3d_shape[2], conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
    "conv_layer4 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu')(conv_layer3)\n",
    "\n",
    "flatten_layer = keras.layers.Flatten()(conv_layer4)\n",
    "\n",
    "## fully connected layers\n",
    "dense_layer1 = keras.layers.Dense(units=256, activation='relu')(flatten_layer)\n",
    "dense_layer1 = keras.layers.Dropout(0.4)(dense_layer1)\n",
    "dense_layer2 = keras.layers.Dense(units=128, activation='relu')(dense_layer1)\n",
    "dense_layer2 = keras.layers.Dropout(0.4)(dense_layer2)\n",
    "output_layer = keras.layers.Dense(units=ytrain.shape[1], activation='softmax')(dense_layer2)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer, name='HybridSN')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(1):\n",
    "    time1 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=128, epochs=100, verbose=0)     \n",
    "    time2 = int(time.time())\n",
    "    \n",
    "    # predict\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('Hybrid:', oa, kappa, ' Time Comsumption:', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClusterCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## vanilla ClusterCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Clusters: (610, 340, 15) Clustering Time:  7.15625 s.  # of Clusters: 100\n",
      "ClusterCNN: 0.9974804540377672 0.9966613864840083  Training Time: 42\n"
     ]
    }
   ],
   "source": [
    "patch = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 100   \n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "start_time = time.process_time()\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "cluster_algorithm = 'minikmeans' \n",
    "\n",
    "mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)\n",
    "    \n",
    "timeused = (time.process_time() - start_time)\n",
    "print(f'{cluster_algorithm}.  Clustering Time:{timeused}s.  # of Clusters:{np.max(clusterlabel)+1}')\n",
    "\n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(1):\n",
    "    time3 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "    time4 = int(time.time())\n",
    "    \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('ClusterCNN:', oa, kappa, ' Training Time:', time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## ClusterCNN with very few Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatchKmeans.  Clustering Time:1.390625s.  # of Clusters:20\n",
      "ClusterCNN: 0.9955583261902906 0.9941154674703562  Training Time: 119.9375\n"
     ]
    }
   ],
   "source": [
    "patch = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 20    \n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "start_time = time.process_time()\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "\n",
    "cluster_algorithm = 'minibatchKmeans'\n",
    "mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)\n",
    "\n",
    "timeused = (time.process_time() - start_time)\n",
    "print(f'{cluster_algorithm}.  Clustering Time:{timeused}s.  # of Clusters:{np.max(clusterlabel)+1}')\n",
    "\n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "\n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "start_time = time.process_time()\n",
    "# training\n",
    "model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)\n",
    "training_time = time.process_time()-start_time\n",
    "\n",
    "# predict\n",
    "y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "oa = accuracy_score(ytest, y_pred)\n",
    "kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "print('ClusterCNN:', oa, kappa, ' Training Time:', training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClusterCNN with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = 25\n",
    "PCsNum = 15  \n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "start_time = time.process_time()\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "\n",
    "cluster_algorithm = 'density' \n",
    "dbcluster = cluster.DBSCAN().fit(HPCs[:,:,:3].reshape((-1, 3)))\n",
    "clusterlabel = dbcluster.labels_.reshape((GT.shape))\n",
    "\n",
    "for i in range(np.max(clusterlabel)+1):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)\n",
    "    \n",
    "timeused = (time.process_time() - start_time)\n",
    "print(f'{cluster_algorithm}.  Clustering Time:{timeused}s.  # of Clusters:{np.max(clusterlabel)+1}')\n",
    "    \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(1):\n",
    "    time3 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "    time4 = int(time.time())\n",
    "    \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('ClusterCNN:', oa, kappa, ' Training Time:', time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## ClusterCNN with Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (207400, 207400) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-882a3514ca5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mcluster_algorithm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'spectral'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mspecluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpectralClustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mClusterNum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHPCs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPCs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mclusterlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClusterNum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\cluster\\spectral.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    482\u001b[0m             self.affinity_matrix_ = pairwise_kernels(X, metric=self.affinity,\n\u001b[0;32m    483\u001b[0m                                                      \u001b[0mfilter_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                                                      **params)\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[1;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown kernel %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m     \u001b[0mK\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# exponentiate K in-place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (207400, 207400) and data type float64"
     ]
    }
   ],
   "source": [
    "patch = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 20   \n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "start_time = time.process_time()\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "\n",
    "cluster_algorithm = 'spectral'\n",
    "specluster = cluster.SpectralClustering(n_clusters=ClusterNum).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = specluster.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)\n",
    "    \n",
    "timeused = (time.process_time() - start_time)\n",
    "print(f'{cluster_algorithm}.  Clustering Time:{timeused}s.  # of Clusters:{np.max(clusterlabel)+1}')\n",
    "    \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(1):\n",
    "    time3 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "    time4 = int(time.time())\n",
    "    \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('ClusterCNN:', oa, kappa, ' Training Time:', time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## ClusterCNN without Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just CNN: 0.9985713914647134 0.9981068585319672  Training Time: 86\n"
     ]
    }
   ],
   "source": [
    "patch = 25\n",
    "PCsNum = 15\n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "clusterstack = HPCs\n",
    "    \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(1):\n",
    "    time3 = int(time.time())\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "    time4 = int(time.time())\n",
    "    \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('Just CNN:', oa, kappa, ' Training Time:', time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClusterCNN with 1ofK Clustering Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatchKmeans Clustering Time:  1.578125 s.  # of Clusters: 15\n",
      "ClusterCNN: 0.9963375672095379 0.9951479675667266  Training Time: 103.640625\n"
     ]
    }
   ],
   "source": [
    "patch = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 15    \n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "cluster_algorithm = 'minibatchKmeans'\n",
    "mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "clusterstack = keras.utils.to_categorical(clusterlabel) \n",
    "timeused = (time.process_time() - start_time)\n",
    "print(cluster_algorithm, 'Clustering Time: ', timeused, 's.  # of Clusters:', np.max(clusterlabel)+1)\n",
    "\n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, ClusterNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "\n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "start_time = time.process_time()\n",
    "# training\n",
    "model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)\n",
    "training_time = time.process_time()-start_time\n",
    "\n",
    "# predict\n",
    "y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "oa = accuracy_score(ytest, y_pred)\n",
    "kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "print('ClusterCNN:', oa, kappa, ' Training Time:', training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UnmixingCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python3 library for basic unmixing functions: https://github.com/etienne-monier/lib-unmixing  \n",
    "Robust-NMF: https://github.com/neel-dey/robust-nmf  \n",
    "ELMM & social norms: https://github.com/ldrumetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于pytorch的unmixing\n",
    "import torch\n",
    "from LibUnmixing import rnmf_torch\n",
    "\n",
    "patch = 25\n",
    "PCsNum = 15\n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HPCs = torch.from_numpy(HPCs*0.05 + 6).float().to(device)\n",
    "_, coeff, _, _ = rnmf_numpy.robust_nmf(data=HPCs.reshape(-1,PCsNum).T, rank=PCsNum, beta=1.5, reg_val=1, sum_to_one=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hevig\\Miniconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.decomposition.nmf module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing rNMF uniformly at random.\n",
      "Iter = 0; Obj = 1475416.9297868833\n",
      "Iter = 100; Obj = 3912.4473343576865; Err = 0.015729731152012108\n",
      "Iter = 200; Obj = 1551.8463062996962; Err = 0.005008916448288215\n",
      "Iter = 300; Obj = 1141.3356810085188; Err = 0.0017754878052168716\n",
      "Iter = 400; Obj = 1016.0074163617171; Err = 0.0007429637962627082\n",
      "Iter = 500; Obj = 963.9826844181962; Err = 0.0003686186787709258\n",
      "Iter = 600; Obj = 937.5786238476079; Err = 0.00020855132760839\n",
      "Iter = 700; Obj = 922.2983822277364; Err = 0.00012919028231047898\n",
      "Iter = 800; Obj = 912.6675624280581; Err = 8.514312254877367e-05\n",
      "Iter = 900; Obj = 906.2314675577193; Err = 5.8779215188313146e-05\n",
      "Iter = 1000; Obj = 901.7264577172554; Err = 4.2302177757944224e-05\n",
      "Maximum number of iterations achieved\n"
     ]
    }
   ],
   "source": [
    "# 基于numpy的unmixing\n",
    "from LibUnmixing import rnmf_numpy\n",
    "\n",
    "patch = 25\n",
    "PCsNum = 15\n",
    "HPCs = applyPCA(HSI,numComponents=PCsNum)\n",
    "HPCs = (HPCs*0.05 + 5).astype(np.float32)\n",
    "_, coeff, _, _ = rnmf_numpy.robust_nmf(data=HPCs.reshape(-1,PCsNum).T, rank=PCsNum, beta=1.5, reg_val=1, sum_to_one=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusterCNN: 0.9376087690589366 0.9175131519863402  Training Time: 110.0\n",
      "ClusterCNN: 0.9561287306163796 0.9420706214834399  Training Time: 107.671875\n",
      "ClusterCNN: 0.9596872646042754 0.9465647237152662  Training Time: 104.84375\n",
      "ClusterCNN: 0.9087508766461466 0.8784307729369  Training Time: 100.296875\n",
      "ClusterCNN: 0.9257902802670199 0.9017708080533953  Training Time: 102.765625\n"
     ]
    }
   ],
   "source": [
    "X, y = createImageCubes(coeff.T.reshape(HPCs.shape), GT, windowSize=patch)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(patch, patch, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "P1 = keras.layers.MaxPool2D()(C1)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(P1) \n",
    "P2 = keras.layers.MaxPool2D()(C2)\n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(P2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=128, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=64, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for i in range(5):\n",
    "    start_time = time.process_time()\n",
    "    # training\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)\n",
    "    training_time = time.process_time()-start_time\n",
    "\n",
    "    # predict\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "    print('ClusterCNN:', oa, kappa, ' Training Time:', training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# PatchSize Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchSize: 7 Clustering Time: 2\n",
      "ClusterCNN: 0.9105691056910569 0.8816097409211088  Training Time: 11\n",
      "PatchSize: 9 Clustering Time: 3\n",
      "ClusterCNN: 0.9655055975479883 0.9543354052535001  Training Time: 12\n",
      "PatchSize: 11 Clustering Time: 3\n",
      "ClusterCNN: 0.9828566975765605 0.9772958877786196  Training Time: 17\n",
      "PatchSize: 13 Clustering Time: 1\n",
      "ClusterCNN: 0.9853502688381517 0.9805807740195652  Training Time: 22\n",
      "PatchSize: 15 Clustering Time: 2\n",
      "ClusterCNN: 0.994986882776176 0.9933615049001331  Training Time: 31\n",
      "PatchSize: 17 Clustering Time: 2\n",
      "ClusterCNN: 0.9963115925088963 0.9951137942897778  Training Time: 36\n",
      "PatchSize: 19 Clustering Time: 3\n",
      "ClusterCNN: 0.9948310345723266 0.9931544969261642  Training Time: 44\n",
      "PatchSize: 21 Clustering Time: 3\n",
      "ClusterCNN: 0.9964934154133873 0.9953554940989772  Training Time: 55\n"
     ]
    }
   ],
   "source": [
    "PCsNum = 15\n",
    "ClusterNum = 150    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "for windowSize in [7, 9, 11, 13, 15, 17, 19, 21]:\n",
    "\n",
    "    time1 = int(time.time())\n",
    "    clusterstack = np.zeros(HPCs.shape)\n",
    "    mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum, n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "    clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "    for i in range(ClusterNum):\n",
    "        clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "    time2 = int(time.time())\n",
    "    print('PatchSize:', windowSize, 'Clustering Time:', time2-time1)\n",
    "\n",
    "    X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "    ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "    ## input layer\n",
    "    IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "    C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "    C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "    ## flatten\n",
    "    FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "    ## fully connected layers\n",
    "    D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "    D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "    D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "    D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "    OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "    model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "    # compiling the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    for j in range(1):\n",
    "        time3 = int(time.time())\n",
    "        # training\n",
    "        model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)\n",
    "        time4 = int(time.time())\n",
    "\n",
    "        # predic\n",
    "        y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "        oa = accuracy_score(ytest, y_pred)\n",
    "        kappa = cohen_kappa_score(ytest, y_pred)\n",
    "\n",
    "        print('ClusterCNN:', oa, kappa, ' Training Time:', time4-time3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Auto Search\n",
    "Grid Search for the opitimal PCA and Clustering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2 0.8412426296786929 132.4375 s\n",
      "5 4 0.9461284708693732 134.296875 s\n",
      "5 8 0.9909608041767318 131.421875 s\n",
      "5 16 0.9928309826229252 131.5625 s\n",
      "5 32 0.9928050079222837 137.109375 s\n",
      "5 64 0.9922855139094522 133.4375 s\n",
      "5 128 0.9963375672095379 136.125 s\n",
      "10 2 0.8413725031819008 171.0 s\n",
      "10 4 0.9817138107483311 174.359375 s\n",
      "10 8 0.985635990545209 173.671875 s\n",
      "10 16 0.9944673887633445 181.21875 s\n",
      "10 32 0.9970388841268605 174.671875 s\n",
      "10 64 0.9962336684069716 176.65625 s\n",
      "10 128 0.9976882516428999 170.25 s\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (42776, 25, 25, 15) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d83504f05de7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mclusterstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHPCs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateImageCubes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusterstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindowSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m345\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-553d73a77646>\u001b[0m in \u001b[0;36mcreateImageCubes\u001b[1;34m(X, y, windowSize, removeZeroLabels)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mpatchIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchIndex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremoveZeroLabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mpatchesData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchesData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpatchesLabels\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mpatchesLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchesLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpatchesLabels\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mpatchesLabels\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (42776, 25, 25, 15) and data type float64"
     ]
    }
   ],
   "source": [
    "windowSize = 17\n",
    "for PCsNum in [5, 10, 15, 20, 25, 30]:\n",
    "    HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "    clusterstack = np.zeros(HPCs.shape)\n",
    "    for ClusterNum in [2, 4, 8, 16, 32, 64, 128]:\n",
    "        start = time.process_time()\n",
    "        \n",
    "        mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum, n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "        clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "        for i in range(ClusterNum):\n",
    "            clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)\n",
    "            \n",
    "        X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "        ytrain = keras.utils.to_categorical(ytrain)\n",
    "        \n",
    "        ## input layer\n",
    "        IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "        C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "        C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "        ## flatten\n",
    "        FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "        ## fully connected layers\n",
    "        D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "        D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "        D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "        D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "        OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "        model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "        # compiling the model\n",
    "        adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        model.fit(x=Xtrain, y=ytrain, batch_size=128, epochs=100, verbose=0)\n",
    "        \n",
    "        # predic\n",
    "        y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "        oa = accuracy_score(ytest, y_pred)\n",
    "        \n",
    "        timeused = (time.process_time() - start)\n",
    "        \n",
    "        print(PCsNum, ClusterNum, oa, timeused,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2 0.7449284396997324 107.109375 s\n",
      "10 4 0.9286994467388764 117.78125 s\n",
      "10 8 0.9786487960726252 112.640625 s\n",
      "10 16 0.9902335125587678 111.78125 s\n",
      "10 32 0.9946232369671939 113.375 s\n",
      "10 48 0.991869918699187 114.828125 s\n",
      "10 64 0.9887789293228395 121.875 s\n",
      "10 80 0.9825969505701447 113.59375 s\n",
      "10 96 0.9911166523805813 109.296875 s\n",
      "10 112 0.9843632302137718 118.546875 s\n",
      "10 128 0.9916101716927712 115.515625 s\n"
     ]
    }
   ],
   "source": [
    "windowSize = 21\n",
    "PCsNum = 10\n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "ClusterNum = [2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128]\n",
    "oa1 = np.zeros(len(ClusterNum))\n",
    "for i in range(len(ClusterNum)):\n",
    "    start = time.process_time()\n",
    "\n",
    "    mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum[i], n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "    clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "    for k in range(ClusterNum[i]):\n",
    "        clusterstack[clusterlabel==k]=np.mean(HPCs[clusterlabel==k], axis=0)\n",
    "\n",
    "    X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "    ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "    ## input layer\n",
    "    IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "    C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "    C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "    ## flatten\n",
    "    FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "    ## fully connected layers\n",
    "    D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "    D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "    D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "    D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "    OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "    model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "\n",
    "    # compiling the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=32, epochs=50, verbose=0)\n",
    "\n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa1[i] = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    timeused = (time.process_time() - start)\n",
    "\n",
    "    print(PCsNum, ClusterNum[i], oa1[i], timeused,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.49, 92.87, 97.86, 99.02, 99.46, 99.19, 98.88, 98.26, 99.11,\n",
       "       98.44, 99.16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(oa1*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100 0.9446998623340866 98.734375 s\n",
      "2 100 0.9822073300605211 79.46875 s\n",
      "5 100 0.9878438400997428 84.625 s\n",
      "10 100 0.988934777526689 106.25 s\n",
      "15 100 0.9918958933998285 125.75 s\n",
      "20 100 0.9932206031325489 157.921875 s\n",
      "25 100 0.9918958933998285 164.640625 s\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (42776, 17, 17, 30) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bd12c81392cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mclusterstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHPCs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateImageCubes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusterstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindowSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m345\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-553d73a77646>\u001b[0m in \u001b[0;36mcreateImageCubes\u001b[1;34m(X, y, windowSize, removeZeroLabels)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mpatchIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchIndex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremoveZeroLabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mpatchesData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchesData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpatchesLabels\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mpatchesLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatchesLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpatchesLabels\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mpatchesLabels\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (42776, 17, 17, 30) and data type float64"
     ]
    }
   ],
   "source": [
    "windowSize = 17\n",
    "ClusterNum = 100\n",
    "PCsNum = [1, 2, 5, 10, 15, 20, 25, 30]\n",
    "oa2 = np.zeros(len(PCsNum))\n",
    "for i in range(len(PCsNum)):\n",
    "    HPCs,pca = applyPCA(HSI,numComponents=PCsNum[i])\n",
    "    clusterstack = np.zeros(HPCs.shape)    \n",
    "    \n",
    "    start = time.process_time()\n",
    "    \n",
    "    mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum, n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "    clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "    for k in range(ClusterNum):\n",
    "        clusterstack[clusterlabel==k]=np.mean(HPCs[clusterlabel==k], axis=0)\n",
    "\n",
    "    X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "    ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "    ## input layer\n",
    "    IL = keras.Input(shape=(windowSize, windowSize, PCsNum[i]), name='IL')\n",
    "    C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "    C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "    ## flatten\n",
    "    FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "    ## fully connected layers\n",
    "    D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "    D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "    D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "    D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "    OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "    model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "\n",
    "    # compiling the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "\n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa2[i] = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    timeused = (time.process_time() - start)\n",
    "\n",
    "    print(PCsNum[i], ClusterNum, oa2[i], timeused,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([94.47, 98.22, 98.78, 98.89, 99.19, 99.32, 99.19,  0.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(oa2*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = i\n",
    "a = range(len(PCsNum))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (207400, 21, 21, 20) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-43256b97da13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mclusterstack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHPCs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclusterlabel\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateImageCubes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusterstack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindowSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m345\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-553d73a77646>\u001b[0m in \u001b[0;36mcreateImageCubes\u001b[1;34m(X, y, windowSize, removeZeroLabels)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mzeroPaddedX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadWithZeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# split patches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mpatchesData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mpatchesLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mpatchIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (207400, 21, 21, 20) and data type float64"
     ]
    }
   ],
   "source": [
    "for i in range(stop, len(PCsNum)):\n",
    "    del Xtrain, Xtest, ytrain, ytest\n",
    "    HPCs,pca = applyPCA(HSI,numComponents=PCsNum[i])\n",
    "    clusterstack = np.zeros(HPCs.shape)    \n",
    "    \n",
    "    start = time.process_time()\n",
    "    \n",
    "    mbk_means = cluster.MiniBatchKMeans(n_clusters=ClusterNum, n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "    clusterlabel = mbk_means.labels_.reshape((GT.shape))\n",
    "    for k in range(ClusterNum):\n",
    "        clusterstack[clusterlabel==k]=np.mean(HPCs[clusterlabel==k], axis=0)\n",
    "\n",
    "    X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "    ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "    ## input layer\n",
    "    IL = keras.Input(shape=(windowSize, windowSize, PCsNum[i]), name='IL')\n",
    "    C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "    C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "    ## flatten\n",
    "    FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "    ## fully connected layers\n",
    "    D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "    D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "    D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "    D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "    OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "\n",
    "    model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "\n",
    "    # compiling the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=64, epochs=100, verbose=0)\n",
    "\n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa2[i] = accuracy_score(ytest, y_pred)\n",
    "\n",
    "    timeused = (time.process_time() - start)\n",
    "\n",
    "    print(PCsNum[i], ClusterNum, oa2[i], timeused,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Stability Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs on different PCA numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 100 0.9773441734417344\n",
      "15 100 0.9700813008130081\n",
      "15 100 0.9765853658536585\n",
      "15 100 0.9799457994579945\n",
      "15 100 0.9795121951219512\n",
      "15 100 0.9814634146341463\n",
      "15 100 0.9768021680216802\n",
      "15 100 0.9706233062330624\n",
      "15 100 0.9742005420054201\n",
      "15 100 0.9757181571815718\n",
      "15 100 0.9712737127371274\n",
      "15 100 0.9713821138211383\n",
      "15 100 0.972140921409214\n",
      "15 100 0.974308943089431\n",
      "15 100 0.974308943089431\n",
      "15 100 0.975609756097561\n",
      "15 100 0.9750677506775067\n",
      "15 100 0.9772357723577236\n",
      "15 100 0.9762601626016261\n",
      "15 100 0.9773441734417344\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 100    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "     \n",
    "\n",
    "for j in range(20):\n",
    "    model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "    # compiling the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 100 0.9731165311653116\n",
      "20 100 0.9737669376693767\n",
      "20 100 0.9751761517615176\n",
      "20 100 0.9689972899728997\n",
      "20 100 0.9718157181571816\n",
      "20 100 0.9773441734417344\n",
      "20 100 0.9779945799457994\n",
      "20 100 0.9785365853658536\n",
      "20 100 0.9731165311653116\n",
      "20 100 0.9650948509485094\n",
      "20 100 0.974959349593496\n",
      "20 100 0.9740921409214092\n",
      "20 100 0.9753929539295393\n",
      "20 100 0.9759349593495935\n",
      "20 100 0.9740921409214092\n",
      "20 100 0.9800542005420054\n",
      "20 100 0.9762601626016261\n",
      "20 100 0.9799457994579945\n",
      "20 100 0.9734417344173442\n",
      "20 100 0.9753929539295393\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 20\n",
    "ClusterNum = 100    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs on different Clustering numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 80 0.9796205962059621\n",
      "15 80 0.9778861788617886\n",
      "15 80 0.9759349593495935\n",
      "15 80 0.9705149051490515\n",
      "15 80 0.9733333333333334\n",
      "15 80 0.9792953929539295\n",
      "15 80 0.966070460704607\n",
      "15 80 0.9737669376693767\n",
      "15 80 0.974308943089431\n",
      "15 80 0.9746341463414634\n",
      "15 80 0.968780487804878\n",
      "15 80 0.9745257452574526\n",
      "15 80 0.9699728997289973\n",
      "15 80 0.9732249322493225\n",
      "15 80 0.9726829268292683\n",
      "15 80 0.9730081300813008\n",
      "15 80 0.9757181571815718\n",
      "15 80 0.9711653116531165\n",
      "15 80 0.9747425474254743\n",
      "15 80 0.977669376693767\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 80    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 200 0.9772357723577236\n",
      "15 200 0.9745257452574526\n",
      "15 200 0.977669376693767\n",
      "15 200 0.9798373983739838\n",
      "15 200 0.9699728997289973\n",
      "15 200 0.9747425474254743\n",
      "15 200 0.9653116531165311\n",
      "15 200 0.9650948509485094\n",
      "15 200 0.9736585365853658\n",
      "15 200 0.9738753387533875\n",
      "15 200 0.9656368563685637\n",
      "15 200 0.968780487804878\n",
      "15 200 0.9719241192411924\n",
      "15 200 0.9750677506775067\n",
      "15 200 0.9791869918699186\n",
      "15 200 0.9747425474254743\n",
      "15 200 0.9724661246612466\n",
      "15 200 0.974308943089431\n",
      "15 200 0.9753929539295393\n",
      "15 200 0.9676964769647697\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 200    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 150 0.9753929539295393\n",
      "15 150 0.9712737127371274\n",
      "15 150 0.9723577235772358\n",
      "15 150 0.9822222222222222\n",
      "15 150 0.980379403794038\n",
      "15 150 0.9794037940379404\n",
      "15 150 0.981680216802168\n",
      "15 150 0.9782113821138212\n",
      "15 150 0.9778861788617886\n",
      "15 150 0.978319783197832\n",
      "15 150 0.978319783197832\n",
      "15 150 0.9709485094850948\n",
      "15 150 0.9811382113821138\n",
      "15 150 0.9782113821138212\n",
      "15 150 0.9769105691056911\n",
      "15 150 0.9815718157181572\n",
      "15 150 0.9771273712737127\n",
      "15 150 0.9796205962059621\n",
      "15 150 0.9808130081300813\n",
      "15 150 0.9820054200542006\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 150    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs on 2DCNN without Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 150 0.9790785907859079\n",
      "15 150 0.9808130081300813\n",
      "15 150 0.9689972899728997\n",
      "15 150 0.9748509485094851\n",
      "15 150 0.9782113821138212\n",
      "15 150 0.9799457994579945\n",
      "15 150 0.9792953929539295\n",
      "15 150 0.9812466124661247\n",
      "15 150 0.9791869918699186\n",
      "15 150 0.9774525745257453\n",
      "15 150 0.9781029810298103\n",
      "15 150 0.9802710027100271\n",
      "15 150 0.9736585365853658\n",
      "15 150 0.9823306233062331\n",
      "15 150 0.9805962059620597\n",
      "15 150 0.9740921409214092\n",
      "15 150 0.9775609756097561\n",
      "15 150 0.9740921409214092\n",
      "15 150 0.981029810298103\n",
      "15 150 0.9778861788617886\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 0\n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "\n",
    "X, y = createImageCubes(HPCs, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs on different CNN structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 150 0.9723577235772358\n",
      "15 150 0.9745257452574526\n",
      "15 150 0.9688888888888889\n",
      "15 150 0.9644444444444444\n",
      "15 150 0.9753929539295393\n",
      "15 150 0.9726829268292683\n",
      "15 150 0.9768021680216802\n",
      "15 150 0.9792953929539295\n",
      "15 150 0.9766937669376694\n",
      "15 150 0.9643360433604337\n",
      "15 150 0.9766937669376694\n",
      "15 150 0.9679132791327913\n",
      "15 150 0.9723577235772358\n",
      "15 150 0.9782113821138212\n",
      "15 150 0.9770189701897019\n",
      "15 150 0.9755013550135502\n",
      "15 150 0.9724661246612466\n",
      "15 150 0.9765853658536585\n",
      "15 150 0.9785365853658536\n",
      "15 150 0.9744173441734417\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 150    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 150 0.9755013550135502\n",
      "15 150 0.9772357723577236\n",
      "15 150 0.971490514905149\n",
      "15 150 0.9740921409214092\n",
      "15 150 0.9678048780487805\n",
      "15 150 0.9785365853658536\n",
      "15 150 0.9757181571815718\n",
      "15 150 0.9784281842818429\n",
      "15 150 0.9728997289972899\n",
      "15 150 0.9787533875338753\n",
      "15 150 0.9760433604336043\n",
      "15 150 0.9777777777777777\n",
      "15 150 0.9747425474254743\n",
      "15 150 0.9731165311653116\n",
      "15 150 0.978319783197832\n",
      "15 150 0.9753929539295393\n",
      "15 150 0.9725745257452575\n",
      "15 150 0.9771273712737127\n",
      "15 150 0.9758265582655826\n",
      "15 150 0.9662872628726287\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 150    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HPCs.reshape((-1, HPCs.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)            \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs on New Clustering method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 150 0.9759349593495935\n",
      "15 150 0.9786449864498645\n",
      "15 150 0.9785365853658536\n",
      "15 150 0.9787533875338753\n",
      "15 150 0.9748509485094851\n",
      "15 150 0.9774525745257453\n",
      "15 150 0.9829810298102981\n",
      "15 150 0.9786449864498645\n",
      "15 150 0.9753929539295393\n",
      "15 150 0.9769105691056911\n",
      "15 150 0.9766937669376694\n",
      "15 150 0.9739837398373984\n",
      "15 150 0.9798373983739838\n",
      "15 150 0.9786449864498645\n",
      "15 150 0.9784281842818429\n",
      "15 150 0.9804878048780488\n",
      "15 150 0.9753929539295393\n",
      "15 150 0.9781029810298103\n",
      "15 150 0.981029810298103\n",
      "15 150 0.9658536585365853\n"
     ]
    }
   ],
   "source": [
    "windowSize = 25\n",
    "PCsNum = 15\n",
    "ClusterNum = 150    \n",
    "HPCs,pca = applyPCA(HSI,numComponents=PCsNum)\n",
    "clusterstack = np.zeros(HPCs.shape)\n",
    "k_means = cluster.KMeans(n_clusters=ClusterNum, n_init=4).fit(HSI.reshape((-1, HSI.shape[2])))\n",
    "clusterlabel = k_means.labels_.reshape((GT.shape))\n",
    "for i in range(ClusterNum):\n",
    "    clusterstack[clusterlabel==i]=np.mean(HPCs[clusterlabel==i], axis=0)        \n",
    "X, y = createImageCubes(clusterstack, GT, windowSize=windowSize)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test_ratio, random_state=345,stratify=y)\n",
    "ytrain = keras.utils.to_categorical(ytrain)\n",
    "\n",
    "## input layer\n",
    "IL = keras.Input(shape=(windowSize, windowSize, PCsNum), name='IL')\n",
    "C1 = keras.layers.Conv2D(filters=64, kernel_size=(5,5), activation='relu', name='C1')(IL)\n",
    "C2 = keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='C2')(C1)        \n",
    "## flatten\n",
    "FL = keras.layers.Flatten(name='FL')(C2)        \n",
    "## fully connected layers\n",
    "D1 = keras.layers.Dense(units=256, activation='relu', name='D1')(FL)\n",
    "D1 = keras.layers.Dropout(0.4, name='dropout1')(D1)\n",
    "D2 = keras.layers.Dense(units=128, activation='relu', name='D2')(D1)\n",
    "D2 = keras.layers.Dropout(0.4, name='dropout2')(D2)\n",
    "OL = keras.layers.Dense(units=ytrain.shape[1], activation='softmax', name='OL')(D2)\n",
    "        \n",
    "model = keras.models.Model(inputs=IL, outputs=OL, name='2DCNN_FCs')\n",
    "        \n",
    "# compiling the model\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "for j in range(20):\n",
    "    model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, verbose=0)     \n",
    "    # predic\n",
    "    y_pred = np.argmax(model.predict(Xtest), axis=1)\n",
    "    oa = accuracy_score(ytest, y_pred)\n",
    "    print(PCsNum, ClusterNum, oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
